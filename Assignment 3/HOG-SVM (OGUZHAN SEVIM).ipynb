
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I had a last minute problem in the general pipeline. My HOG part stopped working and I couldn't figure out why.\n",
    "## So, as a last result, I submit this notebook for my part (Oguzhan Sevim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os # this library provides easy access to the local file directories\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import multilabel_confusion_matrix, precision_recall_fscore_support, plot_confusion_matrix\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HOG2(image, win_size, dummy=1):\n",
    "    ## This function takes an RGB image as input. It calculates HOG features at each keypoint found by SIFT.\n",
    "    max_kp = 500\n",
    "    num_of_bins=30\n",
    "    \n",
    "    # Width and height of the image:\n",
    "    im_h = image.shape[0]\n",
    "    im_w = image.shape[1]\n",
    "    \n",
    "    # Find the coordinates of SIFT keypoints:\n",
    "    sift = cv.SIFT_create()\n",
    "    kp = sift.detect(image,None)\n",
    "    kp = np.array([kp[i].pt for i in range(len(kp))])\n",
    "    \n",
    "    # IF there exists too many keypoints, select random max_kp of them.\n",
    "    if kp.shape[0] > max_kp:\n",
    "        idx = np.random.permutation(kp.shape[0])[0:max_kp]\n",
    "        kp = kp[idx,:]\n",
    "        \n",
    "    # Convert the BGR to grayscale format:\n",
    "    image = cv.cvtColor(image,cv.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate the gradients in x and y axis:\n",
    "    Gradx = cv.Sobel(image,cv.CV_32F,1,0,ksize=3) # By using cv2.CV_32F, gradient of each pixel will be 32-bit floating numbers\n",
    "    Grady = cv.Sobel(image,cv.CV_32F,0,1,ksize=3)\n",
    "    \n",
    "    # Find the angles between the gradients in radians:\n",
    "    GradRadian = np.arctan2(Grady,Gradx) # Each element is between -3.14 and 3.14\n",
    "    \n",
    "    # Create an empty array for the descriptor with proper dimensions:\n",
    "    hist = np.zeros((kp.shape[0] , num_of_bins))\n",
    "    \n",
    "    kp = np.rint(kp)\n",
    "    \n",
    "    for i in range(kp.shape[0]):\n",
    "        kp_coor = kp[i,:]\n",
    "        \n",
    "        # Create a mask around the keypoint i:\n",
    "        mask = np.zeros(image.shape, np.uint8)\n",
    "        mask[int( np.max((kp_coor[1]-win_size,0)) ) : int( np.min((kp_coor[1]+win_size+1,im_h)) ) ,\n",
    "             int( np.max((kp_coor[0]-win_size,0)) ) : int( np.min((kp_coor[0]+win_size+1,im_w)) ) ] = 1\n",
    "        \n",
    "        # Find the histogram of gradients for the window around keypoint i:\n",
    "        hist_window = cv.calcHist([GradRadian],[0],mask,[num_of_bins],[-np.pi,np.pi])\n",
    "        hist_window = hist_window / np.sum(np.abs(hist_window)) # Divide the histogram array by its L1 norm, so it adds up to 1.\n",
    "\n",
    "        # Add the histogram of current cell to the overall descriptor matrix:\n",
    "        hist[i,:] = hist_window[:,0]\n",
    "\n",
    "    return kp, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training data:\n",
    "def read_train(window_size):\n",
    "    train_dir = 'Caltech20_v1/training'\n",
    "    \n",
    "    y_11 = [] # We will collect the labels of dataset in this list\n",
    "    X_11 = [] # This will be our dataset\n",
    "\n",
    "    descs = []\n",
    "\n",
    "    # Keep class names in and corresponding label in this dic.\n",
    "    classNameDic = {}\n",
    "\n",
    "    for cls, class_name in enumerate(os.listdir(train_dir)):\n",
    "        classNameDic.update({class_name : cls})\n",
    "        print('Class name:', class_name)\n",
    "\n",
    "        class_dir = train_dir+'/'+class_name\n",
    "        for im_name in os.listdir(class_dir):\n",
    "\n",
    "            # Save the class label of the image:\n",
    "            y_11.append(cls)\n",
    "\n",
    "            # Read the image as a (x,y,3) numpy array:\n",
    "            image = cv.imread(class_dir+'/'+im_name)\n",
    "\n",
    "            # Find the HOG of the image:\n",
    "            _, histogram = HOG2(image, window_size, None)\n",
    "\n",
    "            X_11.append(list(histogram))\n",
    "            descs = descs + list(histogram)\n",
    "\n",
    "    descs = np.array(descs)\n",
    "    y_11 = np.array(y_11)\n",
    "    \n",
    "    return X_11, y_11, descs, classNameDic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test data:\n",
    "def read_test(window_size, classNameDic):\n",
    "    test_dir = 'Caltech20_v1/testing'\n",
    "\n",
    "    y_11 = [] # We will collect the labels of dataset in this list\n",
    "    X_11 = [] # This will be our dataset\n",
    "\n",
    "    for class_name in os.listdir(test_dir):\n",
    "\n",
    "        cls = classNameDic[class_name]\n",
    "        class_dir = test_dir+'/'+class_name\n",
    "        for im_name in os.listdir(class_dir):\n",
    "\n",
    "            # Save the class label of the image:\n",
    "            y_11.append(cls)\n",
    "\n",
    "            # Read the image as a (x,y,3) numpy array:\n",
    "            image = cv.imread(class_dir+'/'+im_name)\n",
    "\n",
    "            # Find the HOG of the image:\n",
    "            _, histogram = HOG2(image, window_size, None)\n",
    "\n",
    "            X_11.append(list(histogram))\n",
    "\n",
    "    y_11 = np.array(y_11)\n",
    "    return X_11, y_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form the clusters:\n",
    "def cluster_function(n_clusters, descs):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=2, max_iter=50, random_state=0).fit(descs)\n",
    "    return kmeans\n",
    "\n",
    "def quantizer(kmeans, X, n_clusters):\n",
    "    features = []\n",
    "    for i in range(len(X)):\n",
    "        clust_assigns = kmeans.predict(X[i])\n",
    "        feature = np.histogram(clust_assigns, np.arange(n_clusters+1))\n",
    "        features.append(list(feature[0]))\n",
    "\n",
    "    X = np.array(features)\n",
    "    X = X / np.sum(np.abs(X) , axis=1).reshape((X.shape[0],1))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KfoldSVM(X_11,y_11,X_test_11,y_test_11,kf,c,k):\n",
    "    mean_accuracy = 0\n",
    "    for train_index, cv_index in kf.split(X_11, y_11):\n",
    "        X_train, X_cv = X_11[train_index], X_11[cv_index]\n",
    "        y_train, y_cv = y_11[train_index], y_11[cv_index]\n",
    "    \n",
    "        model = SVC(C=c, kernel='linear', class_weight='balanced')\n",
    "        model.fit(X_train, y_train)\n",
    "        fold_accuracy = model.score(X_cv, y_cv)\n",
    "        mean_accuracy += fold_accuracy\n",
    "        \n",
    "    model = SVC(C=c, kernel='linear', class_weight='balanced')\n",
    "    model.fit(X_11, y_11)\n",
    "    test_accuracy = model.score(X_test_11, y_test_11)\n",
    "    return mean_accuracy / k , test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class name: airplanes\n",
      "Class name: anchor\n",
      "Class name: background_class\n",
      "Class name: barrel\n",
      "Class name: camera\n",
      "Class name: car_side\n",
      "Class name: dalmatian\n",
      "Class name: Faces\n",
      "Class name: ferry\n",
      "Class name: headphone\n",
      "Class name: lamp\n",
      "Class name: pizza\n",
      "Class name: pyramid\n",
      "Class name: snoopy\n",
      "Class name: soccer_ball\n",
      "Class name: stop_sign\n",
      "Class name: strawberry\n",
      "Class name: sunflower\n",
      "Class name: water_lilly\n",
      "Class name: windsor_chair\n",
      "Class name: yin_yang\n",
      "Data is read\n",
      "Dictionary is found\n",
      "Data is quantized\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'precision_recall_fscore_support' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-5db8207df283>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Find the required evaluation metrics:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mF1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msupport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mMeanF1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mconf_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultilabel_confusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'precision_recall_fscore_support' is not defined"
     ]
    }
   ],
   "source": [
    "# Read the training and test data:\n",
    "X_first, y, descs, classNameDic = read_train(12)\n",
    "X_test_first, y_test = read_test(12, classNameDic)\n",
    "print('Data is read')\n",
    "\n",
    "# Find the dictionary and quantize the features:\n",
    "kmeans = cluster_function(275, descs)\n",
    "print('Dictionary is found')\n",
    "X = quantizer(kmeans, X_first, 275)\n",
    "X_test = quantizer(kmeans, X_test_first, 275)\n",
    "print('Data is quantized')\n",
    "\n",
    "# Build the SVM model and train it:\n",
    "model = SVC(C=300, kernel='linear', class_weight='balanced')\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict the test labels:\n",
    "y_predict = model.predict(X_test)\n",
    "\n",
    "# Find the required evaluation metrics:\n",
    "precision, recall, F1, support = precision_recall_fscore_support(y_test, y_predict)\n",
    "MeanF1 = np.mean(F1)\n",
    "conf_matrix = multilabel_confusion_matrix(y_test, y_predict)\n",
    "accuracy = 100 * np.sum(y_predict == y_test) / y_test.shape[0]\n",
    "\n",
    "# Print the metrics:\n",
    "print(\"MeanF1:\", MeanF1)\n",
    "print(\"precision:\", precision)\n",
    "print(\"recall:\",recall)\n",
    "print(\"F1:\", F1)\n",
    "print(\"conf_matrix:\", conf_matrix)\n",
    "print(\"accuracy:\", accuracy)\n",
    "\n",
    "plot_confusion_matrix(model, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeanF1: 0.4612430308419902\n",
      "precision: [0.74074074 0.28571429 0.         0.71428571 0.6        0.82608696\n",
      " 0.6        0.58064516 1.         0.3        0.11111111 0.625\n",
      " 0.7        0.5        0.         1.         0.33333333 0.71428571\n",
      " 0.37037037 0.88235294 0.5       ]\n",
      "recall: [1.   0.5  0.   0.25 0.15 0.95 0.45 0.9  0.55 0.3  0.05 0.5  0.35 0.65\n",
      " 0.   0.6  0.25 0.5  0.5  0.75 0.15]\n",
      "F1: [0.85106383 0.36363636 0.         0.37037037 0.24       0.88372093\n",
      " 0.51428571 0.70588235 0.70967742 0.3        0.06896552 0.55555556\n",
      " 0.46666667 0.56521739 0.         0.75       0.28571429 0.58823529\n",
      " 0.42553191 0.81081081 0.23076923]\n",
      "conf_matrix: [[[353   7]\n",
      "  [  0  20]]\n",
      "\n",
      " [[335  25]\n",
      "  [ 10  10]]\n",
      "\n",
      " [[337  43]\n",
      "  [  0   0]]\n",
      "\n",
      " [[358   2]\n",
      "  [ 15   5]]\n",
      "\n",
      " [[358   2]\n",
      "  [ 17   3]]\n",
      "\n",
      " [[356   4]\n",
      "  [  1  19]]\n",
      "\n",
      " [[354   6]\n",
      "  [ 11   9]]\n",
      "\n",
      " [[347  13]\n",
      "  [  2  18]]\n",
      "\n",
      " [[360   0]\n",
      "  [  9  11]]\n",
      "\n",
      " [[346  14]\n",
      "  [ 14   6]]\n",
      "\n",
      " [[352   8]\n",
      "  [ 19   1]]\n",
      "\n",
      " [[354   6]\n",
      "  [ 10  10]]\n",
      "\n",
      " [[357   3]\n",
      "  [ 13   7]]\n",
      "\n",
      " [[347  13]\n",
      "  [  7  13]]\n",
      "\n",
      " [[369  11]\n",
      "  [  0   0]]\n",
      "\n",
      " [[360   0]\n",
      "  [  8  12]]\n",
      "\n",
      " [[350  10]\n",
      "  [ 15   5]]\n",
      "\n",
      " [[356   4]\n",
      "  [ 10  10]]\n",
      "\n",
      " [[343  17]\n",
      "  [ 10  10]]\n",
      "\n",
      " [[358   2]\n",
      "  [  5  15]]\n",